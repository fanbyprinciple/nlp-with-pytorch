{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\"In this example, weâ€™ll show how to download, tokenize, and train a model on the IMDb reviews dataset. This task takes the text of a review and requires the model to predict whether the sentiment of the review is positive or negative. Letâ€™s start by downloading the dataset from the Large Movie Review Dataset webpage.\"\n\n    - Hugging face tutorial (https://huggingface.co/transformers/master/custom_datasets.html)\n\n![](data:image/png;base64,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)","metadata":{}},{"cell_type":"markdown","source":"### 1. Getting the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:25.482357Z","iopub.execute_input":"2021-07-02T01:58:25.482683Z","iopub.status.idle":"2021-07-02T01:58:25.992101Z","shell.execute_reply.started":"2021-07-02T01:58:25.482659Z","shell.execute_reply":"2021-07-02T01:58:25.991089Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This data is organized into pos and neg folders with one text file per example. Letâ€™s write a function that can read this in.","metadata":{}},{"cell_type":"code","source":"def give_me_text_and_labels(input_csv=\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"):\n    df = pd.read_csv(input_csv)\n    \n    df['label'] = [1 if x==\"positive\" else 0 for x in df['sentiment'] ]\n    return df['review'].values, df['label'].values\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-02T01:58:25.993465Z","iopub.execute_input":"2021-07-02T01:58:25.993664Z","iopub.status.idle":"2021-07-02T01:58:25.999022Z","shell.execute_reply.started":"2021-07-02T01:58:25.993643Z","shell.execute_reply":"2021-07-02T01:58:25.997734Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"texts, labels = give_me_text_and_labels()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.001481Z","iopub.execute_input":"2021-07-02T01:58:26.001814Z","iopub.status.idle":"2021-07-02T01:58:26.529812Z","shell.execute_reply.started":"2021-07-02T01:58:26.001783Z","shell.execute_reply":"2021-07-02T01:58:26.528875Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"texts[0][:100], labels[0], len(labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.531300Z","iopub.execute_input":"2021-07-02T01:58:26.531617Z","iopub.status.idle":"2021-07-02T01:58:26.536288Z","shell.execute_reply.started":"2021-07-02T01:58:26.531587Z","shell.execute_reply":"2021-07-02T01:58:26.535612Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"(\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The\",\n 1,\n 50000)"},"metadata":{}}]},{"cell_type":"code","source":"train_length = 40000\ntrain_texts, train_labels = texts[:train_length], labels[:train_length]\ntest_texts, test_labels = texts[train_length:], labels[train_length:]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.537099Z","iopub.execute_input":"2021-07-02T01:58:26.537288Z","iopub.status.idle":"2021-07-02T01:58:26.557723Z","shell.execute_reply.started":"2021-07-02T01:58:26.537268Z","shell.execute_reply":"2021-07-02T01:58:26.557113Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"We now have a train and test dataset, but letâ€™s also also create a validation set which we can use for for evaluation and tuning without tainting our test set results. Sklearn has a convenient utility for creating such splits:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.560860Z","iopub.execute_input":"2021-07-02T01:58:26.561186Z","iopub.status.idle":"2021-07-02T01:58:26.580763Z","shell.execute_reply.started":"2021-07-02T01:58:26.561162Z","shell.execute_reply":"2021-07-02T01:58:26.579932Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer\n\nAlright, weâ€™ve read in our dataset. Now letâ€™s tackle tokenization. Weâ€™ll eventually train a classifier using pre-trained DistilBert, so letâ€™s use the DistilBert tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.582217Z","iopub.execute_input":"2021-07-02T01:58:26.582523Z","iopub.status.idle":"2021-07-02T01:58:28.936638Z","shell.execute_reply.started":"2021-07-02T01:58:26.582501Z","shell.execute_reply":"2021-07-02T01:58:28.935187Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Now we can simply pass our texts to the tokenizer. Weâ€™ll pass `truncation=True` and `padding=True`, which will ensure that all of our sequences are padded to the same length and are truncated to be no longer modelâ€™s maximum input length. This will allow us to feed batches of sequences into the model at the same time.","metadata":{}},{"cell_type":"code","source":"type(train_texts), type(list(train_texts))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:28.940271Z","iopub.execute_input":"2021-07-02T01:58:28.940503Z","iopub.status.idle":"2021-07-02T01:58:28.948607Z","shell.execute_reply.started":"2021-07-02T01:58:28.940482Z","shell.execute_reply":"2021-07-02T01:58:28.947690Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"(numpy.ndarray, list)"},"metadata":{}}]},{"cell_type":"code","source":"train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:28.949946Z","iopub.execute_input":"2021-07-02T01:58:28.950155Z","iopub.status.idle":"2021-07-02T01:58:50.837609Z","shell.execute_reply.started":"2021-07-02T01:58:28.950129Z","shell.execute_reply":"2021-07-02T01:58:50.836660Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### 3. Creating a dataset object\n\nNow, letâ€™s turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a torch.utils.data.Dataset object and implementing __len__ and __getitem__. In TensorFlow, we pass our input encodings and labels to the from_tensor_slices constructor method. We put the data in this format so that the data can be easily batched such that each key in the batch encoding corresponds to a named parameter of the forward() method of the model we will train.","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass IMDBdataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:50.838716Z","iopub.execute_input":"2021-07-02T01:58:50.838996Z","iopub.status.idle":"2021-07-02T01:58:50.845047Z","shell.execute_reply.started":"2021-07-02T01:58:50.838968Z","shell.execute_reply":"2021-07-02T01:58:50.844546Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"train_dataset = IMDBdataset(train_encodings, train_labels)\ntest_dataset = IMDBdataset(test_encodings,test_labels)\nval_dataset = IMDBdataset(val_encodings, val_labels)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:50.845818Z","iopub.execute_input":"2021-07-02T01:58:50.846094Z","iopub.status.idle":"2021-07-02T01:58:51.333571Z","shell.execute_reply.started":"2021-07-02T01:58:50.846072Z","shell.execute_reply":"2021-07-02T01:58:51.332878Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# idx = 0\n# # print(train_encodings.items())\n# item = {key: torch.tensor(val[idx]) for key, val in train_encodings.items()}\n# print(item)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.334571Z","iopub.execute_input":"2021-07-02T01:58:51.334794Z","iopub.status.idle":"2021-07-02T01:58:51.338780Z","shell.execute_reply.started":"2021-07-02T01:58:51.334767Z","shell.execute_reply":"2021-07-02T01:58:51.338089Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"Now that our datasets our ready, we can fine-tune a model either with the ðŸ¤— Trainer/TFTrainer or with native PyTorch/TensorFlow.","metadata":{}},{"cell_type":"markdown","source":"### 4. Fine Tuning with native pytorch\n\nThe steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model to fine-tune, define the TrainingArguments/TFTrainingArguments and instantiate a Trainer/TFTrainer.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import DistilBertForSequenceClassification, AdamW\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.339655Z","iopub.execute_input":"2021-07-02T01:58:51.339985Z","iopub.status.idle":"2021-07-02T01:58:51.353384Z","shell.execute_reply.started":"2021-07-02T01:58:51.339960Z","shell.execute_reply":"2021-07-02T01:58:51.352623Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\nmodel = model.to(device=device)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.354219Z","iopub.execute_input":"2021-07-02T01:58:51.354453Z","iopub.status.idle":"2021-07-02T01:58:53.857217Z","shell.execute_reply.started":"2021-07-02T01:58:51.354426Z","shell.execute_reply":"2021-07-02T01:58:53.856592Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:53.858547Z","iopub.execute_input":"2021-07-02T01:58:53.859077Z","iopub.status.idle":"2021-07-02T01:58:53.868156Z","shell.execute_reply.started":"2021-07-02T01:58:53.859038Z","shell.execute_reply":"2021-07-02T01:58:53.866908Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:53.869567Z","iopub.execute_input":"2021-07-02T01:58:53.869831Z","iopub.status.idle":"2021-07-02T01:58:55.004950Z","shell.execute_reply.started":"2021-07-02T01:58:53.869803Z","shell.execute_reply":"2021-07-02T01:58:55.003770Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"optim = AdamW(model.parameters(),lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.006658Z","iopub.execute_input":"2021-07-02T01:58:55.006893Z","iopub.status.idle":"2021-07-02T01:58:55.022033Z","shell.execute_reply.started":"2021-07-02T01:58:55.006870Z","shell.execute_reply":"2021-07-02T01:58:55.021121Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"This is the all important training loop. It is giving me meory limit exceeded error! if it happens to you, comment the training loop.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfor epoch in range(3):\n    for batch in tqdm(train_dataloader):\n        optim.zero_grad()\n        input_ids= batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n    print(f\"Loss for epoch {epoch} is {loss}\")\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.023155Z","iopub.execute_input":"2021-07-02T01:58:55.023397Z","iopub.status.idle":"2021-07-02T01:58:55.034953Z","shell.execute_reply.started":"2021-07-02T01:58:55.023350Z","shell.execute_reply":"2021-07-02T01:58:55.034154Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"once trained save it","metadata":{}},{"cell_type":"code","source":"save_directory = \"./\"","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.035884Z","iopub.execute_input":"2021-07-02T01:58:55.036107Z","iopub.status.idle":"2021-07-02T01:58:55.048427Z","shell.execute_reply.started":"2021-07-02T01:58:55.036085Z","shell.execute_reply":"2021-07-02T01:58:55.047542Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.049359Z","iopub.execute_input":"2021-07-02T01:58:55.049618Z","iopub.status.idle":"2021-07-02T01:58:55.553797Z","shell.execute_reply.started":"2021-07-02T01:58:55.049573Z","shell.execute_reply":"2021-07-02T01:58:55.552558Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"and loading it back","metadata":{}},{"cell_type":"code","source":"# from transformers import TFAutoModel, AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n# model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.555088Z","iopub.execute_input":"2021-07-02T01:58:55.555479Z","iopub.status.idle":"2021-07-02T01:58:55.559742Z","shell.execute_reply.started":"2021-07-02T01:58:55.555445Z","shell.execute_reply":"2021-07-02T01:58:55.558397Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"### 5. Building the classifier and using sequence selection\n\nNow lets do something that is not covered in the official tutorial","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nclassifier =pipeline('sentiment-analysis',model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.561027Z","iopub.execute_input":"2021-07-02T01:58:55.561253Z","iopub.status.idle":"2021-07-02T01:58:55.582148Z","shell.execute_reply.started":"2021-07-02T01:58:55.561230Z","shell.execute_reply":"2021-07-02T01:58:55.580980Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"sample_test= test_dataset[0]\nsample_test","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.585744Z","iopub.execute_input":"2021-07-02T01:58:55.585975Z","iopub.status.idle":"2021-07-02T01:58:55.605886Z","shell.execute_reply.started":"2021-07-02T01:58:55.585953Z","shell.execute_reply":"2021-07-02T01:58:55.605432Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101,  2034,  2125,  1045,  2215,  2000,  2360,  2008,  1045,  8155,\n          4314,  2006,  1996,  2576,  4094,  1998,  1045,  2179,  1996,  3185,\n          5805,  1012,  1045,  3266,  2000,  3422,  1996,  2878, 28844,  5643,\n         29591,  1997,  1037,  2143,  1012,  2023,  3185,  7545,  1037,  2659,\n          2000,  2434,  4784,  1012,  2748,  2009,  2001,  2434,  2947,  2026,\n          1016,  3340,  2612,  1997,  1015,  1012,  2024,  2256,  2143,  4898,\n          2008,  4895, 16748,  8082,  2008,  2027,  2064,  2069,  2272,  2039,\n          2007,  2023,  1029,  1029,  3772,  2001,  9202,  1010,  1998,  1996,\n          3494,  2020,  4406,  3085,  2005,  1996,  2087,  2112,  1012,  1996,\n          2599,  3203,  1999,  1996,  2466,  2018,  2053,  2204, 11647,  2012,\n          2035,  1012,  2027,  2081,  2014, 28939,  2046,  2070,  4066,  1997,\n          1037,  2919,  3124,  1998,  1045,  2106,  2025,  2156,  2008,  2012,\n          2035,  1012,  2672,  1045,  4771,  2242,  1010,  1045,  2079,  2025,\n          2113,  1012,  2002,  2001,  1996,  2087,  2091,  2000,  3011,  1010,\n          7882,  2839,  1999,  1996,  3185,  1012,  1045,  2106,  2025,  5806,\n          2041,  2151,  2769,  2005,  2023, 13044,  1012,  1045,  2471,  4299,\n          9004,  2050,  2052,  2272,  2000,  1996,  5343,  1997,  2023,  9643,\n          1010,  5805,  3185,  1998,  2433,  1037,  6186,  1012, 19424,  2008,\n          2015,  2035,  1045,  2031,  2000,  2360,  4902,   999,   102,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'labels': tensor(0)}"},"metadata":{}}]},{"cell_type":"code","source":"classifier(\"I love it\")[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.607352Z","iopub.execute_input":"2021-07-02T01:58:55.607689Z","iopub.status.idle":"2021-07-02T01:58:55.638496Z","shell.execute_reply.started":"2021-07-02T01:58:55.607659Z","shell.execute_reply":"2021-07-02T01:58:55.637594Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"{'label': 'LABEL_0', 'score': 0.518386721611023}"},"metadata":{}}]},{"cell_type":"code","source":"test_texts[1001], test_labels[1001]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.639625Z","iopub.execute_input":"2021-07-02T01:58:55.639923Z","iopub.status.idle":"2021-07-02T01:58:55.645182Z","shell.execute_reply.started":"2021-07-02T01:58:55.639891Z","shell.execute_reply":"2021-07-02T01:58:55.644209Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"('Acting was weak, but in a horror flick, I can live with that if the story is good. It wasn\\'t. The initial event was an clumsy and obvious ploy to exploit most people\\'s adoration of kids. OK, fine. Fast forward to the \"place in the country\" where they will recover emotionally. I like the revelation of the ghosts. OK, cool--this will be a supernatural kinda horror story, with rotting things partly in our world partly in...where ever. Then the action starts pulling like a three headed dog in a flurry of cats and birds--Is there an evil force trying to attack them directly? Is there an evil force trying to attack them INdirectly--make people do awful things they wouldn\\'t really do? Oh, wait, no, maybe the whole REGION is some kind of psychic echo chamber where ambient discord can reverberate into murder? OK, hold on--maybe it\\'s really just one little mentally tangled \"Delbert\"-style redneck boy who misses his Mommy and is on some kind of spree like a K-Tel Norman Bates knock off? Oh, yeah--extra points off: the only Black character seems to be the grandson of an \"Our Gang\" pullman porter. The actor plays it as straight as he can given the crummy dialogue, but the fact is, his purpose is \"Y\\'all done betta get outa heah, Boss!\" At least they wrote him smart enough to GTF outta there. The bit with the little girl being silenced and pulled away was definitely creepy, as was the chick in the shower. Those were just two of quite a few really delicious tidbits in this movie. The problem is that they are combined in disharmonious ways, like a bite of steak, a bite of chocolate and a bite of a Gummi bear. Each is great on it\\'s own, but mixed up? Bleah! Such potential. Wasted.',\n 0)"},"metadata":{}}]},{"cell_type":"code","source":"classifier(test_texts[1001])[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.646327Z","iopub.execute_input":"2021-07-02T01:58:55.646655Z","iopub.status.idle":"2021-07-02T01:58:56.841485Z","shell.execute_reply.started":"2021-07-02T01:58:55.646625Z","shell.execute_reply":"2021-07-02T01:58:56.840568Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"{'label': 'LABEL_0', 'score': 0.5119705200195312}"},"metadata":{}}]}]}